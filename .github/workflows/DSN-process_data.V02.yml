name: DSN Process SQM/TESS Raw Data and Upload to Box
# Trigger the workflow on a schedule (in production mode)
on:
  schedule:
    - cron: '0 8 * * 3'  # Runs every Wed at 08:00 UTC
  workflow_dispatch:
  
env:
  INFLUX_TOKEN: ${{ secrets.INFLUX_TOKEN1 }}
  BOX_CONFIG: ${{ secrets.BOX_CONFIG }}
  BOX_PATH: "DSNdata/BOX/"
  MERGE_PATH: "DSNdata/MERGE/"
  BOX_ARCHIVE_ID: "304428997491"

jobs:
  process_and_upload:
    runs-on: ubuntu-latest

    steps:
      # Checkout Repository, get python
      - name: Checkout Repository
        uses: actions/checkout@v3

      # Check for files in DSNdata/NEW, quit if none
      - name: Fail if no NEW files
        run: |
          FILES=$(ls -1 DSNdata/NEW)
          if [ -z "$FILES" ]; then
            CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')
            echo $CURRENT_DATE "No files in DSNdata/NEW." >> DSNdata/RUN_LOG
            echo "Exit: No files found in DSNdata/NEW." >&2
            exit 1
          fi

      # Step 1: Setup environment
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')
          echo $CURRENT_DATE "Start DSN-process_data." >> DSNdata/RUN_LOG
          sudo apt-get update
          sudo apt-get install -y pkg-config libgtk-3-dev libc-bin curl
          pip install --upgrade pip
          pip install -r requirements.txt

      # Step 2: run DSN_V03.py on all files in DSNdata/NEW
      - name: Process files, write results to DSNdata/INFLUX
        run: |
          find DSNdata/NEW -maxdepth 1 -type f ! -name '.*' | while IFS= read -r file; do
            if [ -f "$file" ]; then  # ignore dirs
              new_file="$file"
              echo "Process $new_file"
              python3 DSN_V03.py $new_file || { echo "Error processing $new_file" >&2; exit 1; }
            fi
          done
  
      # Step 2b: Split INFLUX CSVs into ~3-year chunks (to avoid Influx partition buffering limits)
      - name: Split INFLUX CSVs into 3-year chunks
        run: |
          set -euo pipefail
          mkdir -p DSNdata/INFLUX_CHUNKS

          python3 - <<'PY'
          import os, glob, csv

          IN_DIR  = "DSNdata/INFLUX"
          OUT_DIR = "DSNdata/INFLUX_CHUNKS"
          os.makedirs(OUT_DIR, exist_ok=True)

          TIME_KEYS = ("UTC", "time", "_time")

          def year_from_ts(s):
              try:
                  return int(str(s)[:4])
              except Exception:
                  return None

          def is_meta_row(row0):
              # Handle BOM and whitespace before '#'
              if row0 is None:
                  return False
              s = str(row0).strip()
              s = s.lstrip('\ufeff')
              return s.startswith("#")

          def split_csv(path):
              base = os.path.basename(path).rsplit(".", 1)[0]

              buckets = {}
              meta_lines = []
              header = None
              time_idx = None
              any_rows = False

              def bucket_for_year(y):
                  start = (y // 3) * 3
                  end = start + 2
                  return (start, end)

              with open(path, "r", encoding="utf-8", newline="") as f:
                  rdr = csv.reader(f)
                  for row in rdr:
                      if not row:
                          continue

                      if is_meta_row(row[0]):
                          if header is None:
                              meta_lines.append(row)
                          continue

                      if header is None:
                          header = row
                          time_idx = None
                          for k in TIME_KEYS:
                              if k in header:
                                  time_idx = header.index(k)
                                  break
                          if time_idx is None:
                              raise RuntimeError(f"No UTC/time column found in {path}. Header={header}")
                          continue

                      if len(row) <= time_idx:
                          continue
                      y = year_from_ts(row[time_idx])
                      if y is None:
                          continue
                      any_rows = True
                      key = bucket_for_year(y)
                      buckets.setdefault(key, []).append(row)

              if not any_rows:
                  return

              for (start, end), rows in sorted(buckets.items()):
                  out_path = os.path.join(OUT_DIR, f"{base}_{start}-{end}.csv")
                  with open(out_path, "w", encoding="utf-8", newline="") as out:
                      w = csv.writer(out)
                      for m in meta_lines:
                          w.writerow(m)
                      w.writerow(header)
                      w.writerows(rows)
                  print(f"[chunk] {os.path.basename(path)} -> {os.path.basename(out_path)} ({len(rows)} rows)")

          for p in sorted(glob.glob(os.path.join(IN_DIR, "*.csv"))):
              split_csv(p)
          PY


      # Step 3: Set Up Docker
      - name: Set Up Docker
        run: |
          echo "Installing Docker..."
          sudo apt-get update
          sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
          curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
          sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
          sudo apt-get update
          sudo apt-get remove -y containerd
          sudo apt-get install -y docker-ce
          sudo systemctl start docker
          sudo systemctl enable docker
          docker --version

      # Step 4: Run InfluxDB CLI in Docker
      - name: Start InfluxDB CLI Docker Container
        run: |
          echo "Pulling InfluxDB CLI Docker image..."
          docker pull influxdb:2.7.5
          echo "Starting InfluxDB CLI container..."
          docker run --name influx-cli -d influxdb:2.7.5 tail -f /dev/null

      # Step 5: Copy 3-year chunked CSV Files to Docker Container
      - name: Copy chunked CSV Files to Docker
        run: |
          echo "Copying chunked .csv files to Docker container..."
          docker exec influx-cli mkdir -p /data/chunks
          docker cp DSNdata/INFLUX_CHUNKS/. influx-cli:/data/chunks/

      # Step 6: Upload CSV Files to InfluxDB Cloud
      - name: Upload CSV Files to InfluxDB Cloud
        run: |
          echo "Running InfluxDB CLI commands inside Docker..."
          IN_FILES=$(docker exec influx-cli ls /data/chunks | grep .csv || true)
          if [ -z "$IN_FILES" ]; then
            echo "Error: No .csv files found in /data to upload." >&2
            exit 1
          else
            echo "Influx files in docker: "$IN_FILES  
          fi
          for infile in $IN_FILES; do
            success=false
            for attempt in {1..5}; do
              echo "Uploading /data/chunks/$infile to InfluxDB Cloud..."
              docker exec influx-cli influx write \
              --host https://us-east-1-1.aws.cloud2.influxdata.com \
              --org DSN \
              --bucket DSNdata \
              --token $INFLUX_TOKEN \
              --file /data/chunks/$infile \
              --format csv \
              --debug && success=true && break
              echo "Attempt $attempt failed, retrying..."
              sleep 10
            done
            if [ "$success" = false ]; then
              echo "Error: Failed to upload $infile to InfluxDB Cloud." >&2
              exit 1
            fi
          done
          CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')
          echo $CURRENT_DATE "Uploaded chunk files "$IN_FILES" to influx" >> DSNdata/RUN_LOG
          echo "All files successfully uploaded to InfluxDB Cloud."

      # Step 7: Install Box CLI
      - name: Install Box CLI
        run: |
          echo "Installing Box CLI via npm..."
          sudo apt-get update
          sudo apt-get install -y libsecret-1-dev
          npm install -g @box/cli
          box --version

      # Step 8: Boxing...
      - name: Upload Files to Box
        run: |
          echo "$BOX_CONFIG" > box_config.json
          echo "BOX_PATH is set to: ${BOX_PATH}"

          if [ -d "$BOX_PATH" ] && [ "$(ls -A $BOX_PATH)" ]; then
            IN_FILES=$(ls -1 "${BOX_PATH}")
          else
            echo "Warning: No files found in ${BOX_PATH}"
            IN_FILES=""
          fi
          
          if [ -z "$IN_FILES" ]; then
            echo "No files to upload to Box, skipping..."
          else
            echo "Configure and upload to Box."
            box configure:environments:add box_config.json -n "github-box"

            echo "++++++++++ IN_FILES: "$IN_FILES" in ${BOX_PATH}"
            for file in $IN_FILES; do
              box_file="$BOX_PATH$file"
              raw_file="${file/-[0-9][0-9]/}"
              cp $box_file $raw_file
              echo "Boxing $box_file"
              # Extract Box File ID from CSV
              box folders:items $BOX_ARCHIVE_ID --csv > results.csv
              box_ID=$(awk -F ',' -v fname="$raw_file" 'NR > 1 && $9 == fname {print $2}' results.csv)
              echo "Extracted Box ID: '$box_ID'"
              if [ -z "$box_ID" ] || [ "$box_ID" == "null" ]; then
                echo "ðŸ”ŽðŸ”ŽðŸ”Ž File $raw_file not found, upload it"
                box files:upload "$raw_file" -p $BOX_ARCHIVE_ID -y -q
                echo "â¬†ï¸â¬†ï¸â¬†ï¸ Uploaded $raw_file"
              else
                echo "â¬‡ï¸â¬‡ï¸â¬‡ï¸ Download Box File ID: $box_ID"
                box files:download "$box_ID" --destination $MERGE_PATH -y
                echo "âœ…âœ…âœ… Downloaded $box_ID"
                python DSN-box_merge.py "$MERGE_PATH$raw_file" "$raw_file" || { echo "Error merging $raw_file" >&2; exit 1; }
                box files:delete $box_ID -f -y
                echo "Uploading MERGED $raw_file to Box..."
                box files:upload "$MERGE_PATH$raw_file" -p $BOX_ARCHIVE_ID -y -q
                echo "â¬†ï¸â¬†ï¸â¬†ï¸ Uploaded $MERGE_PATH$raw_file"
              fi
            done
            CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')
            echo "$CURRENT_DATE Uploaded $IN_FILES to Box" >> DSNdata/RUN_LOG
            echo "$CURRENT_DATE Uploaded $IN_FILES to Box: written to log."
          fi

      # Step 9: Clean Up
      - name: Clean Up
        run: |
          echo "Delete temporary files..."
          rm -f box_config.json
          rm -f results.csv
          rm -rf DSNdata/NEW/*
          rm -f DSNdata/INFLUX/*
          rm -f DSNdata/INFLUX_CHUNKS/*
          rm -f DSNdata/BOX/*
          rm -f DSNdata/MERGE/*
          echo "Cleaning up Docker container..."
          docker rm -f influx-cli || true
  
      # Step 10: Commit Changes to DSNdata
      - name: Commit Changes to Repository
        run: |
          echo "Checking for changes in DSNdata..."
          CURRENT_DATE=$(date '+%Y-%m-%d %H:%M:%S')
          echo $CURRENT_DATE "Deleted temp files." >> DSNdata/RUN_LOG
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add DSNdata/
          if git diff-index --quiet HEAD; then
            echo "No changes to commit."
          else
            git commit -m "Updated DSNdata"
            git push
          fi
